{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        # self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)  # Output corresponds to num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class VGG11Model(nn.Module):\n",
    "    # Implement VGG11 model for transfer learning\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = models.vgg11(pretrained=True)\n",
    "        \n",
    "        # Freeze the convolutional base\n",
    "        # for param in self.model.features.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        # Replace avgpool with AdaptiveAvgPool2d\n",
    "        self.model.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Replace the classifier with a new one\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)  # Output corresponds to num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKEND:  Agg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Partition the data and create the dataloaders.\"\"\"\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, Grayscale, ToTensor\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Chuyển sang backend không cần GUI\n",
    "\n",
    "print('BACKEND: ', matplotlib.get_backend())\n",
    "NUM_WORKERS = 10\n",
    "# def get_custom_dataset(data_path: str = \"/media/namvq/Data/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     transform = Compose([\n",
    "#         Resize((100, 100)),\n",
    "#         Grayscale(num_output_channels=1),\n",
    "#         ToTensor()\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=transform)\n",
    "#     return trainset, testset\n",
    "\n",
    "# def get_custom_dataset(data_path: str = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Kích thước ảnh cho EfficientNet\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=transform)\n",
    "#     return trainset, testset\n",
    "\n",
    "# def get_custom_dataset(data_path: str = \"/media/namvq/Data/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     train_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Kích thước ảnh cho EfficientNet\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     test_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Kích thước ảnh cho EfficientNet\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=train_transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=test_transform)\n",
    "#     return trainset, testset\n",
    "def get_custom_dataset(data_path: str = \"/media/namvq/Data/chest_xray\"):\n",
    "    \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),  # Kích thước ảnh cho VGG\n",
    "        transforms.RandomAffine(degrees=0, shear=10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.2, 0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "                             [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((150, 150)),  # Kích thước ảnh cho VGG\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "                             [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "    ])\n",
    "    trainset = ImageFolder(os.path.join(data_path, 'train'), transform=train_transform)\n",
    "    testset = ImageFolder(os.path.join(data_path, 'test'), transform=test_transform)\n",
    "    return trainset, testset\n",
    "\n",
    "#Lay tap val goc co 16 anh thoi\n",
    "def get_val_dataloader(batch_size: int = 10):\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    valset = ImageFolder(os.path.join(\"/media/namvq/Data/chest_xray\", 'val'), transform=val_transform)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    return valloader\n",
    "\n",
    "def prepare_dataset_for_centralized_train(batch_size: int, val_ratio: float = 0.1, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    # Split trainset into trainset and valset\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "\n",
    "def prepare_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1, alpha: float = 100, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate non-IID partitions using Dirichlet distribution.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    \n",
    "    # Split trainset into trainset and valset\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "    \n",
    "    # Generate Dirichlet distribution for each class\n",
    "    class_indices = [np.where(train_labels == i)[0] for i in range(len(np.unique(train_labels)))]\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    \n",
    "    for class_idx in class_indices:\n",
    "        np.random.shuffle(class_idx)\n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, num_partitions))\n",
    "        proportions = (np.cumsum(proportions) * len(class_idx)).astype(int)[:-1]\n",
    "        class_partitions = np.split(class_idx, proportions)\n",
    "        for i in range(num_partitions):\n",
    "            partition_indices[i].extend(class_partitions[i])\n",
    "    \n",
    "    # Create Subsets for each partition\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "    \n",
    "    # Split valset into partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "\n",
    "    valsets = random_split(valset, partition_len_val, torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Create DataLoaders for each partition\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Calculate class distribution for each partition in trainloaders\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_partitioned_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1, num_labels_each_party: int = 1, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate partitions where each party has a fixed number of labels.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()  # Load datasets\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels\n",
    "    num_labels = len(np.unique(train_labels))  # Assuming labels are 0 and 1 for binary classification\n",
    "    times = [0 for i in range(num_labels)]\n",
    "    contain = []\n",
    "    #Phan label cho cac client\n",
    "    for i in range(num_partitions):\n",
    "        current = [i%num_labels]\n",
    "        times[i%num_labels] += 1\n",
    "        if num_labels_each_party > 1:\n",
    "            current.append(1-i%num_labels)\n",
    "            times[1-i%num_labels] += 1\n",
    "        contain.append(current)\n",
    "    print(times)\n",
    "    print(contain)\n",
    "    # Create Subsets for each partition\n",
    "\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    for i in range(num_labels):\n",
    "        idx_i = np.where(train_labels == i)[0]  # Get indices of label i in train_labels\n",
    "        idx_i = [trainset.indices[j] for j in idx_i]  # Convert indices to indices in trainset\n",
    "        # #print label of idx_i\n",
    "        # print(\"Label of idx: \", i)\n",
    "        # for j in range(len(idx_i)):\n",
    "        #     idx_in_dataset = trainset.indices[idx_i[j]]\n",
    "        #     print(trainset.dataset.targets[idx_in_dataset])\n",
    "        np.random.shuffle(idx_i)\n",
    "        split = np.array_split(idx_i, times[i])\n",
    "        ids = 0\n",
    "        for j in range(num_partitions):\n",
    "            if i in contain[j]:\n",
    "                partition_indices[j].extend(split[ids])\n",
    "                ids += 1\n",
    "    \n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    # #print label of client 0\n",
    "    # print(\"Client 0\")\n",
    "    # for i in range(len(trainsets[0])):\n",
    "    #     print(trainsets[0][i][1])\n",
    "\n",
    "    # Split valset into partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Create DataLoaders for each partition\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Calculate class distribution for each partition in trainloaders\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_imbalance_label_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 0.5, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate partitions where each party has a fixed number of labels.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()  # Load datasets\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels\n",
    "    num_labels = len(np.unique(train_labels))  # Assuming labels are 0 and 1 for binary classification\n",
    "    min_size = 0\n",
    "    min_require_size = 2\n",
    "\n",
    "    N = len(trainset)\n",
    "\n",
    "\n",
    "    while(min_size < min_require_size):\n",
    "        partition_indices = [[] for _ in range(num_partitions)]\n",
    "        for label in range(num_labels):\n",
    "            idx_label = np.where(train_labels == label)[0]\n",
    "            idx_label = [trainset.indices[j] for j in idx_label]\n",
    "            np.random.shuffle(idx_label)\n",
    "\n",
    "            proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "            # proportions = np.array( [p * len(idx_j) < N/num_partitions] for p, idx_j in zip(proportions, partition_indices))\n",
    "            proportions = np.array([p if p * len(idx_j) < N / num_partitions else 0 for p, idx_j in zip(proportions, partition_indices)])\n",
    "\n",
    "            proportions = proportions / np.sum(proportions)\n",
    "            proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "\n",
    "            partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, np.split(idx_label, proportions))]\n",
    "            min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "        \n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "\n",
    "def apply_gaussian_noise(tensor, std_dev):\n",
    "    noise = torch.randn_like(tensor) * std_dev\n",
    "    return tensor + noise\n",
    "\n",
    "# Hàm đảo ngược chuẩn hóa\n",
    "def unnormalize_image(image_tensor, mean, std):\n",
    "    # Đảo ngược Normalize: (image * std) + mean\n",
    "    for t, m, s in zip(image_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Thực hiện từng kênh\n",
    "    return image_tensor\n",
    "\n",
    "# Hàm hiển thị ảnh từ một tensor\n",
    "def display_image(image_tensor, mean, std):\n",
    "    # Đảo ngược chuẩn hóa\n",
    "    image_tensor = unnormalize_image(image_tensor, mean, std)\n",
    "    # Chuyển tensor thành NumPy array và điều chỉnh thứ tự kênh màu (CHW -> HWC)\n",
    "    image_numpy = image_tensor.permute(1, 2, 0).numpy()\n",
    "    # Cắt giá trị ảnh về phạm vi [0, 1] để hiển thị đúng\n",
    "    image_numpy = image_numpy.clip(0, 1)\n",
    "    # Trả về ảnh NumPy\n",
    "    return image_numpy\n",
    "\n",
    "def prepare_noise_based_imbalance(num_partitions: int, batch_size: int, val_ratio: float = 0.1, sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Chia du lieu ngau nhien va deu cho cac ben, sau do them noise vao cac ben\n",
    "    moi ben i co noise khac nhau Gauss(0, sigma*i/N)\n",
    "    \"\"\"\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    indices = trainset.indices\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    partition_indices = np.array_split(indices, num_partitions)\n",
    "\n",
    "    train_partitions = []\n",
    "\n",
    "    for i, part_indices in enumerate(partition_indices):\n",
    "        partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "        partition_set = Subset(trainset.dataset, part_indices)\n",
    "        \n",
    "        noisy_samples = [apply_gaussian_noise(sample[0], partition_std_dev) for sample in partition_set]\n",
    "        noisy_dataset = [(noisy_samples[j], trainset.dataset[part_indices[j]][1]) for j in range(len(part_indices))]\n",
    "        # train_partitions.append((noisy_samples, [sample[1] for sample in partition_set]))\n",
    "        train_partitions.append(noisy_dataset)\n",
    "    trainloaders = [DataLoader(train_partitions[i], batch_size=batch_size, shuffle=True, num_workers=4) for i in range(num_partitions)]\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "####\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    #Lưu ảnh nhiễu vào running_outputs\n",
    "    # Mean và std từ Normalize\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # Tạo thư mục lưu ảnh nếu chưa tồn tại\n",
    "    output_dir = \"running_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Khởi tạo một lưới 10x6 để hiển thị ảnh\n",
    "    fig, axes = plt.subplots(10, 6, figsize=(15, 25))\n",
    "\n",
    "    # Duyệt qua 60 trainloaders và hiển thị ảnh đầu tiên\n",
    "    for i, trainloader in enumerate(trainloaders[:num_partitions]):\n",
    "        # Lấy ảnh đầu tiên từ trainloader\n",
    "        image_tensor = trainloader.dataset[0][0].clone()  # Clone để tránh thay đổi dữ liệu gốc\n",
    "        \n",
    "        # Tìm vị trí hàng, cột trong lưới\n",
    "        row, col = divmod(i, 6)\n",
    "        plt.sca(axes[row, col])  # Đặt trục hiện tại là vị trí hàng, cột trong lưới\n",
    "        \n",
    "        # Hiển thị ảnh\n",
    "        image_numpy = display_image(image_tensor, mean, std)\n",
    "        axes[row, col].imshow(image_numpy)\n",
    "        axes[row, col].axis('off')\n",
    "    plt.title(f\"Noise image with sigma from {sigma * 1 / num_partitions} to {sigma}\")\n",
    "    # Điều chỉnh layout để không bị chồng lấn\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Lưu ảnh thay vì hiển thị\n",
    "    output_path = os.path.join(output_dir, \"image_noise.png\")\n",
    "    plt.savefig(output_path, dpi=300)  # Lưu ảnh với chất lượng cao\n",
    "\n",
    "    plt.close()  # Đóng figure\n",
    "\n",
    "    print(f\"Ảnh đã được lưu tại {output_path}\")\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "###\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "def prepare_quantity_skew_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 10, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    all_indices = trainset.indices\n",
    "\n",
    "    min_size = 0\n",
    "    while min_size < 1:\n",
    "        proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "        proportions = (np.cumsum(proportions) * len(all_indices)).astype(int)[:-1]\n",
    "\n",
    "        partition_indices = np.split(all_indices, proportions)\n",
    "\n",
    "        min_size = min([len(partition) for partition in partition_indices])\n",
    "        print('Partition sizes:', [len(partition) for partition in partition_indices])\n",
    "        print('Min partition size:', min_size)\n",
    "\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "def load_datasets(\n",
    "    config: DictConfig,\n",
    "    num_clients: int,\n",
    "    val_ratio: float = 0.1,\n",
    "    seed: Optional[int] = 42,\n",
    ") -> Tuple[List[DataLoader], List[DataLoader], DataLoader]:\n",
    "    \"\"\"Create the dataloaders to be fed into the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config: DictConfig\n",
    "        Parameterises the dataset partitioning process\n",
    "    num_clients : int\n",
    "        The number of clients that hold a part of the data\n",
    "    val_ratio : float, optional\n",
    "        The ratio of training data that will be used for validation (between 0 and 1),\n",
    "        by default 0.1\n",
    "    seed : int, optional\n",
    "        Used to set a fix seed to replicate experiments, by default 42\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[DataLoader, DataLoader, DataLoader]\n",
    "        The DataLoaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    print(f\"Dataset partitioning config: {config}\")\n",
    "    batch_size = -1\n",
    "    print('config:' , config)\n",
    "    if \"batch_size\" in config:\n",
    "        batch_size = config.batch_size\n",
    "    elif \"batch_size_ratio\" in config:\n",
    "        batch_size_ratio = config.batch_size_ratio\n",
    "    else:\n",
    "        raise ValueError\n",
    "    partitioning = \"\"\n",
    "    \n",
    "    if \"partitioning\" in config:\n",
    "        partitioning = config.partitioning\n",
    "\n",
    "    # partition the data\n",
    "    if partitioning == \"imbalance_label\":\n",
    "        return prepare_partitioned_dataset(num_clients, batch_size, val_ratio, config.labels_per_client, config.seed)\n",
    "\n",
    "    if partitioning == \"imbalance_label_dirichlet\":\n",
    "        return prepare_imbalance_label_dirichlet(num_clients, batch_size, val_ratio, config.alpha, config.seed)\n",
    "\n",
    "    if partitioning == \"noise_based_imbalance\":\n",
    "        return prepare_noise_based_imbalance(num_clients, batch_size, val_ratio, config.sigma, config.seed)\n",
    "\n",
    "    if partitioning == \"quantity_skew_dirichlet\":\n",
    "        return prepare_quantity_skew_dirichlet(num_clients, batch_size, val_ratio, config.alpha, config.seed)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim \n",
    "import copy\n",
    "import random \n",
    "import numpy as np\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def reset_model_to_zero(model):\n",
    "    for param in model.parameters():\n",
    "        param.data.fill_(0.0)\n",
    "\n",
    "def federated_train(trainloaders, valloaders, testloader, config):\n",
    "    #Cai dat seed\n",
    "    random.seed(config.dataset_seed)\n",
    "    np.random.seed(config.dataset_seed)\n",
    "    torch.manual_seed(config.dataset_seed)\n",
    "    torch.cuda.manual_seed(config.dataset_seed)\n",
    "    torch.cuda.manual_seed_all(config.dataset_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # model = ResNet18(num_classes=2)\n",
    "    model = VGG11Model(num_classes=2)\n",
    "    nets = {net_i: copy.deepcopy(model) for net_i in range(len(trainloaders))}\n",
    "    global_model = copy.deepcopy(model)  # Bản sao mô hình toàn cục\n",
    "    valloader_goc = get_val_dataloader()\n",
    "\n",
    "    num_rounds = config.num_rounds  # Số vòng huấn luyện\n",
    "    accs_test = []\n",
    "    accs_val = []\n",
    "    accs_test.append(evaluate(global_model, testloader))\n",
    "    accs_val.append(evaluate(global_model, valloader_goc))\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Round {round_num + 1}/{num_rounds}\")\n",
    "        start = time.time()\n",
    "        global_para = global_model.state_dict()\n",
    "\n",
    "        # Chọn các client tham gia vào mỗi round\n",
    "        selected_clients = select_clients(trainloaders, config.clients_per_round)\n",
    "        \n",
    "        # Huấn luyện trên các client đã chọn\n",
    "        for client in selected_clients:\n",
    "            nets[client].load_state_dict(global_para)\n",
    "        local_train_net_fedprox(nets, selected_clients, global_model, config, trainloaders, device=DEVICE)\n",
    "\n",
    "        total_data_points = sum([len(trainloaders[client].dataset) for client in selected_clients])\n",
    "        freqs = [len(trainloaders[client].dataset) / total_data_points for client in selected_clients]\n",
    "\n",
    "        for idx in range(len(selected_clients)):\n",
    "            net_para = nets[selected_clients[idx]].cpu().state_dict()\n",
    "            if idx == 0:\n",
    "                for key in net_para:\n",
    "                    global_para[key] = net_para[key] * freqs[idx]\n",
    "            else:\n",
    "                for key in net_para:\n",
    "                    global_para[key] += net_para[key] * freqs[idx]\n",
    "        global_model.load_state_dict(global_para)\n",
    "        global_model.to('cpu')\n",
    "        acc_test = evaluate(global_model, testloader)        \n",
    "        accs_test.append(acc_test)\n",
    "\n",
    "        acc_val = evaluate(global_model, valloader_goc)\n",
    "        accs_val.append(acc_val)\n",
    "\n",
    "        print(f\"Round {round_num + 1} Test Accuracy: {acc_test:.2f}%\")\n",
    "        print(f\"Round {round_num + 1} Val Accuracy: {acc_val:.2f}%\")\n",
    "\n",
    "        if round_num >= 0:\n",
    "            if acc_val > 80.0:\n",
    "                config.learning_rate = 1e-8\n",
    "                print(f\"Accuracy > 80%, decreasing learning rate to {config.learning_rate}\")\n",
    "            elif acc_val > 70.0:\n",
    "                config.learning_rate = 1e-7\n",
    "                print(f\"Accuracy > 70%, decreasing learning rate to {config.learning_rate}\")\n",
    "            elif acc_val > 60.0:\n",
    "                config.learning_rate = 1e-6\n",
    "                print(f\"Accuracy > 60%, decreasing learning rate to {config.learning_rate}\")\n",
    "            elif acc_val > 50.0:\n",
    "                config.learning_rate = 1e-5\n",
    "                print(f\"Accuracy > 50%, decreasing learning rate to {config.learning_rate}\")\n",
    "            else :\n",
    "                config.learning_rate = 1e-4\n",
    "                print(f\"Accuracy <= 50%, increasing learning rate to {config.learning_rate}\")\n",
    "\n",
    "        end = time.time()\n",
    "        print(f'Time for round {round_num + 1}: ', end-start)\n",
    "    # plot_accuracy(accs)\n",
    "    print('accuracies test: ', accs_test)\n",
    "    print('accuracies val: ', accs_val)\n",
    "    plt.plot(range(0, num_rounds + 1), accs_test, marker='o', label='Accuracy_test')\n",
    "    plt.plot(range(0, num_rounds + 1), accs_val, marker='x', label='Accuracy_val')\n",
    "    plt.xlabel('Round')\n",
    "    plt.xticks(range(0, num_rounds + 1, 10))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('WOW WOW WOW')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig('running_outputs/accuracy_summary.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def local_train_net_fedprox(nets, selected_clients, global_model, config, trainloaders, device='cpu'):\n",
    "    for net_id in selected_clients:\n",
    "        net = nets[net_id]\n",
    "        net.to(device)\n",
    "        train_net_fedprox(net, global_model, trainloaders[net_id], config, device=device)\n",
    "\n",
    "\n",
    "def train_net_fedprox(net, global_model, trainloader, config, device):\n",
    "    optimizer = optim.SGD(\n",
    "        filter(lambda p: p.requires_grad, net.parameters()),\n",
    "        lr=config.learning_rate,\n",
    "        momentum=config.momentum\n",
    "    )\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    global_weights = list(global_model.to(device).parameters())\n",
    "    mu = config.mu\n",
    "    for _ in range(config.num_epochs):\n",
    "        for data, target in trainloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Tính toán phần regularization\n",
    "            reg_loss = 0.0\n",
    "            for param_index, param in enumerate(net.parameters()):\n",
    "                reg_loss += ((mu / 2) * torch.norm((param - global_weights[param_index])) ** 2)\n",
    "            loss += reg_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    net.to('cpu')\n",
    "      \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def select_clients(trainloaders, clients_per_round):\n",
    "    \"\"\"Chọn ngẫu nhiên một số client tham gia huấn luyện trong mỗi round.\"\"\"\n",
    "    # Số lượng client có sẵn\n",
    "    total_clients = len(trainloaders)\n",
    "    # Chọn ngẫu nhiên một số client\n",
    "    selected_clients = random.sample(range(total_clients), clients_per_round)\n",
    "    return selected_clients\n",
    "\n",
    "\n",
    "def evaluate(model, testloader):\n",
    "    \"\"\"Đánh giá mô hình trên tập kiểm tra.\"\"\"\n",
    "    # print('evaluate on', device)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()  # Chuyển sang chế độ đánh giá\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    model.to('cpu')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKEND:  Agg\n",
      "Loaded Config:\n",
      "{'num_clients': 60, 'num_epochs': 5, 'batch_size': 10, 'clients_per_round': 6, 'fraction_fit': 0.1, 'learning_rate': 0.001, 'num_rounds': 10, 'partitioning': 'imbalance_label_dirichlet', 'dataset_name': 'chest_xray', 'dataset_seed': 42, 'alpha': 0.5, 'sigma': 0.1, 'labels_per_client': 2, 'momentum': 0.9, 'weight_decay': 1e-05, 'dataset': {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}}\n",
      "Dataset partitioning config: {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}\n",
      "config: {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}\n",
      "Partition 0 class distribution: {1: 5}\n",
      "Partition 1 class distribution: {1: 19}\n",
      "Partition 2 class distribution: {0: 61, 1: 8}\n",
      "Partition 3 class distribution: {0: 29, 1: 1}\n",
      "Partition 4 class distribution: {1: 56}\n",
      "Partition 5 class distribution: {0: 30, 1: 3}\n",
      "Partition 6 class distribution: {1: 41}\n",
      "Partition 7 class distribution: {0: 12, 1: 83}\n",
      "Partition 8 class distribution: {1: 142, 0: 19}\n",
      "Partition 9 class distribution: {0: 10, 1: 1}\n",
      "Partition 10 class distribution: {0: 58, 1: 1}\n",
      "Partition 11 class distribution: {0: 5, 1: 4}\n",
      "Partition 12 class distribution: {1: 40, 0: 67}\n",
      "Partition 13 class distribution: {1: 136}\n",
      "Partition 14 class distribution: {0: 37, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "from data_utils import load_datasets\n",
    "from omegaconf import OmegaConf  # Thêm OmegaConf\n",
    "import os\n",
    "\n",
    "\n",
    "def load_config():\n",
    "    \"\"\"Load configuration using OmegaConf from a dictionary.\"\"\"\n",
    "    config_dict = {\n",
    "        \"num_clients\": 60,\n",
    "        \"num_epochs\": 5,\n",
    "        \"batch_size\": 10,\n",
    "        \"clients_per_round\": 6,\n",
    "        \"fraction_fit\": 0.1,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"num_rounds\": 10,\n",
    "        \"partitioning\": \"imbalance_label_dirichlet\",\n",
    "        \"dataset_name\": \"chest_xray\",\n",
    "        \"dataset_seed\": 42,\n",
    "        \"alpha\": 0.5,\n",
    "        \"sigma\": 0.1,\n",
    "        \"labels_per_client\": 2,  # only used when partitioning is label quantity\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.00001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"${dataset_name}\",\n",
    "            \"partitioning\": \"${partitioning}\",\n",
    "            \"batch_size\": \"${batch_size}\",  # batch_size = batch_size_ratio * total_local_data_size\n",
    "            \"val_split\": 0.0,\n",
    "            \"seed\": \"${dataset_seed}\",\n",
    "            \"alpha\": \"${alpha}\",\n",
    "            \"sigma\": \"${sigma}\",\n",
    "            \"labels_per_client\": \"${labels_per_client}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Chuyển đổi dictionary thành DictConfig\n",
    "    config = OmegaConf.create(config_dict)\n",
    "\n",
    "    return config\n",
    "\n",
    "def main():\n",
    "    # Parse arguments\n",
    "    # args = parse_args()\n",
    "\n",
    "\n",
    "    # Load configuration file\n",
    "    config = load_config()  # Trả về DictConfig\n",
    "\n",
    "    # Kiểm tra các tham số được thay thế chính xác\n",
    "    print(\"Loaded Config:\")\n",
    "    print(config)\n",
    "\n",
    "    # Load dataset\n",
    "    # trainloaders, valloaders, testloader = load_datasets(config.dataset.name, args.num_clients)\n",
    "    trainloaders, valloaders, testloader = load_datasets(\n",
    "        config=config.dataset,\n",
    "        num_clients=config.num_clients,\n",
    "        val_ratio=config.dataset.val_split,\n",
    "    )\n",
    "\n",
    "    # Train federated model\n",
    "    federated_train(trainloaders, valloaders, testloader, config)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
